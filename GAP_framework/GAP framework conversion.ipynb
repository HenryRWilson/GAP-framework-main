{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55546595",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1832/3691994909.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#__init__.py conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeatureless_random_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "#__init__.py conversion\n",
    "\n",
    "from .utils import featureless_random_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "302a3f1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch._C'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1832/1471357488.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# noqa: F403\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mparameter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mParameter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUninitializedParameter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUninitializedBuffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataParallel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIdentity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBilinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLazyLinear\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConv1d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mConvTranspose1d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConvTranspose2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConvTranspose3d\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mLazyConv1d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLazyConv2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLazyConv3d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLazyConvTranspose1d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLazyConvTranspose2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLazyConvTranspose3d\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mParameter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\parameter.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_disabled_torch_function_impl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch._C'"
     ]
    }
   ],
   "source": [
    "#layers.py conversion\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import random \n",
    "from torch.nn import init\n",
    "from utils import featureless_random_graph,get_feature_func,get_neigh_list\n",
    "\"\"\"\n",
    "Here we will implement the GraphSAGE layer and a graph convolutional layer.\n",
    "                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^ TODO\n",
    "Recommend reading \n",
    "The original GraphSAGE paper:\n",
    "https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf\n",
    "\n",
    "Influenced heavily by https://github.com/williamleif/graphsage-simple/\n",
    "\n",
    "TERMINOLOGY:\n",
    "    A graph is described by a set G = {V,E} of nodes and edges.\n",
    "    A representation of a node is any feature associated to that node. I avoid\n",
    "    using 'feature' generally here as this definition also applies to neural\n",
    "    network produced features of nodes ie. trainable embeddings.\n",
    "    - In this case the representation is initially the node features (PCA) for\n",
    "      layer one, and for later layers is the output of the previous layer.\n",
    "\n",
    "We implement a GraphSAGE layer as 2 distinct objects:\n",
    "1. A layer-like Encoder class, with an associated weight matrix W and activation\n",
    "   function. This level takes a representation of a node and combines it with\n",
    "   another representation generated by the Aggregator.\n",
    "2. An Aggregator class which generates a representation of a node from it's\n",
    "   neighbours representations in the previous layer. Can be seen as a learnt\n",
    "   representation producing function which can take some complex-NN forms if needed.\n",
    "This two object layout is useful as the aggregator functions can vary.\n",
    "\n",
    "TODO : Can implement more aggregators\n",
    "\"\"\"\n",
    "class MeanAggregator(nn.Module):\n",
    "    \"\"\"\n",
    "    Aggregates a node's embedding using mean of neighbors' embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self,features,cuda=False,verbose=False,\n",
    "                    gcn=False):\n",
    "        \"\"\"\n",
    "        Inits an aggregator for this graph.\n",
    "        features -- A function mapping a long tensor of node ids to \n",
    "                    a float tensor of feature values.\n",
    "        cuda     -- Whether to cast tensors in this scope to the GPU\n",
    "        \"\"\"\n",
    "        super(MeanAggregator,self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.cuda = cuda\n",
    "        self.verbose = verbose\n",
    "        self.gcn = gcn\n",
    "\n",
    "    def forward(self, nodes, neighbours, num_sample=5):\n",
    "        \"\"\"\n",
    "        The aggregator needs:\n",
    "        nodes -- list of nodes in a batch\n",
    "        neighbours -- list of sets, i-th set is i-th nodes neighbours.\n",
    "        num_sample -- n_samples to draw from the neighbourhood\n",
    "                   ^> None means use full neighbourhood!\n",
    "        \"\"\"\n",
    "        # Making these pointers makes them faster \n",
    "        _set = set\n",
    "        _sample = random.sample \n",
    "        # First let's find neighbours we need to sample\n",
    "        # We sample a distinct set for each node.\n",
    "        if num_sample is not None: # if performing sampling\n",
    "            samp_neighs = []\n",
    "            for neighbourhood in neighbours:\n",
    "                if len(neighbourhood) >= num_sample:\n",
    "                    # If there are more neighbours than samples in a\n",
    "                    # neighbourhood we sample it\n",
    "                    samp_neighs.append(_set(_sample(neighbourhood,num_sample)))\n",
    "                else: # If less then we just take as is\n",
    "                    samp_neighs.append(neighbourhood)\n",
    "        else: \n",
    "            samp_neighs = neighbours \n",
    "        # Add self loops \n",
    "        if self.gcn:\n",
    "            samp_neighs = [samp_neigh + set([nodes[i]]) for i, samp_neigh in\n",
    "                           enumerate(samp_neighs)]\n",
    "        # Find all unique nodes\n",
    "        unique_nodes_list = list(set.union(*samp_neighs)) \n",
    "        # Assign all current nodes an index accessed by their node_id (n) \n",
    "        # in a dict.\n",
    "        unique_nodes = {n:i for i,n in enumerate(unique_nodes_list)}\n",
    "        # Create a mask which will allow the layer to only show data from connected\n",
    "        # nodes to other connected nodes. This has an index for each node in the\n",
    "        # batch with an index for each unique node it may be connected to\n",
    "        mask = torch.zeros(len(samp_neighs),len(unique_nodes))\n",
    "        # Next we find the indices of the maks for the edges\n",
    "        # (This is essentially a condensed adjacency matrix)\n",
    "        column_indices = []\n",
    "        row_indices = []\n",
    "        # samp_neighs is an list of sets of indices representing edges\n",
    "        for i,connected_nodes in enumerate(samp_neighs):\n",
    "            n_neighs = len(connected_nodes)\n",
    "            for n in connected_nodes:\n",
    "                # Network Ordering -> Batch Ordering\n",
    "                column_indices.append(unique_nodes[n])\n",
    "            # Associated indices\n",
    "            for j in range(n_neighs):\n",
    "                row_indices.append(i)\n",
    "        # Set the value of these nodes to 1\n",
    "        mask[row_indices,column_indices] = 1\n",
    "        # Send to GPU if needed\n",
    "        if self.cuda:\n",
    "            mask = mask.cuda()\n",
    "        # Calculate the number of neigbours for each node\n",
    "        num_neigh = mask.sum(1,keepdim=True)\n",
    "        # Divide each 1 in the mask by the number of neighbours\n",
    "        # This is a step towards calculating the mean!\n",
    "        mask = mask.div(num_neigh)\n",
    "        # Get the features of all nodes involved:\n",
    "        # !!! This call recursively calls all previous aggregators in the\n",
    "        # !!! training process\n",
    "        if self.cuda:\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list).cuda())\n",
    "        else:\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list))\n",
    "        # Matrix multiplication here produces the aggregate averaged features\n",
    "        to_feats = mask.mm(embed_matrix.float())\n",
    "        return to_feats\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes nodes.\n",
    "\n",
    "    Feature_dim can be seen as input shape.\n",
    "    Embed_dim can be seen as output shape.\n",
    "    Features is a function that returns a representation of a node\n",
    "    \"\"\" \n",
    "    def __init__(self, features, feature_dim, \n",
    "            embed_dim, adj_lists, aggregator,\n",
    "            num_sample=10,\n",
    "            base_model=None, cuda=False, \n",
    "            feature_transform=False,verbose=False): \n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.feat_dim = feature_dim\n",
    "        self.adj_lists = adj_lists\n",
    "        self.aggregator = aggregator\n",
    "        self.num_sample = num_sample\n",
    "        self.verbose = verbose\n",
    "        self.activation = \"relu\" # TODO : Implement other functions\n",
    "        if base_model != None:\n",
    "            self.base_model = base_model\n",
    "        self.embed_dim = embed_dim\n",
    "        self.cuda = cuda\n",
    "        self.aggregator.cuda = cuda\n",
    "        # The weight is the learnt parameter of this layer used to highlight\n",
    "        # the important regions of the incoming representation.\n",
    "        # It is 2 x feat_dim as it uses two representations at once, the local\n",
    "        # and neighbourhood aggregated.\n",
    "        self.weight = nn.Parameter(\n",
    "                torch.FloatTensor(embed_dim, 2 * self.feat_dim))\n",
    "        init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        \"\"\"\n",
    "        Generates embeddings for a batch of nodes.\n",
    "        nodes     -- list of nodes\n",
    "        \"\"\"\n",
    "        # Calculate aggregated representations\n",
    "        neigh_feats = self.aggregator.forward(nodes, \n",
    "                                              [self.adj_lists[int(node)] for \n",
    "                                               node in nodes], \n",
    "                                              self.num_sample)\n",
    "        # Cast these to GPU if necessary\n",
    "        if self.cuda:\n",
    "            self_feats = self.features(torch.LongTensor(nodes).cuda())\n",
    "        else:\n",
    "            self_feats = self.features(torch.LongTensor([*nodes]))\n",
    "        # Concatenate the layers previous representation with current\n",
    "        combined = torch.cat([self_feats, neigh_feats], dim=1)\n",
    "        # Feed this through an activation function\n",
    "        if self.activation == \"relu\":\n",
    "            combined = F.relu(self.weight.mm(combined.t().float()))\n",
    "        return combined\n",
    "\n",
    "class GraphSAGEEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes nodes.\n",
    "\n",
    "    Feature_dim can be seen as input shape.\n",
    "    Embed_dim can be seen as output shape.\n",
    "    Features is a function that returns a representation of a node\n",
    "    \"\"\" \n",
    "    def __init__(self, features, feature_dim, \n",
    "            embed_dim, adj_lists, aggregator,\n",
    "            num_sample=10,\n",
    "            base_model=None, cuda=False, \n",
    "            feature_transform=False,verbose=False): \n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.feat_dim = feature_dim\n",
    "        self.adj_lists = adj_lists\n",
    "        self.aggregator = aggregator\n",
    "        self.num_sample = num_sample\n",
    "        self.verbose = verbose\n",
    "        self.activation = \"relu\" # TODO : Implement other functions\n",
    "        if base_model != None:\n",
    "            self.base_model = base_model\n",
    "        self.embed_dim = embed_dim\n",
    "        self.cuda = cuda\n",
    "        self.aggregator.cuda = cuda\n",
    "        self.weight = nn.Parameter(\n",
    "                torch.FloatTensor(embed_dim, 2 * self.feat_dim))\n",
    "        init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        \"\"\"\n",
    "        Generates embeddings for a batch of nodes.\n",
    "        nodes     -- list of nodes.\n",
    "\n",
    "        In the graph sage implementation they use a maxpooling on the\n",
    "        neighbourhood state along with including the addition of the aggregated\n",
    "        state into the activation function input.\n",
    "\n",
    "        TODO: Implement max pooling.\n",
    "        \"\"\"\n",
    "        # Calculate aggregated representations\n",
    "        neigh_feats = self.aggregator.forward(nodes, \n",
    "                                              [self.adj_lists[int(node)] for \n",
    "                                               node in nodes], \n",
    "                                              self.num_sample)\n",
    "        # Cast these to GPU if necessary\n",
    "        if self.cuda:\n",
    "            self_feats = self.features(torch.LongTensor(nodes).cuda())\n",
    "        else:\n",
    "            self_feats = self.features(torch.LongTensor([*nodes]))\n",
    "        # Concatenate the layers previous representation with current\n",
    "        combined = torch.cat([self_feats, neigh_feats], dim=1)\n",
    "        # Feed this through an activation function\n",
    "        if self.activation == \"relu\":\n",
    "            combined = F.relu(self.weight.mm(combined.t().float())+neigh_feats)\n",
    "        return combined\n",
    "\"\"\"\n",
    "Unit tests!\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    adj, degrees, feats = featureless_random_graph(100)\n",
    "    adj_list = get_neigh_list(adj)\n",
    "    node_list = [*range(adj.shape[0])]\n",
    "    features = get_feature_func(node_list,feats)\n",
    "    test_agg = MeanAggregator(features,cuda=True)\n",
    "    test_layer = Encoder(features, \n",
    "                         feats.shape[1],\n",
    "                         5, \n",
    "                         adj_list,\n",
    "                         test_agg,\n",
    "                        num_sample=None)\n",
    "    # Attempt to feed a graph through the layer\n",
    "    for i in range(len(adj_list)):\n",
    "        n_con = len(adj_list[i])\n",
    "        print(\"n_connections:\", n_con)\n",
    "        print(adj_list[i])\n",
    "        if n_con == 0:\n",
    "            print(\"No connections! Skipping...\")\n",
    "            continue\n",
    "        x = test_layer(adj_list[i]) # Feed this a node neighbour list\n",
    "        print(f\"Success for {i}!\")\n",
    "        print(x.shape)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51cf695",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modules.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "from layers import MeanAggregator, Encoder\n",
    "from utils import featureless_random_graph,get_feature_func,get_neigh_list\n",
    "\"\"\"\n",
    "To begin with let's try to implement the GAP-scalefree model\n",
    "which uses GraphSAGE layers.\n",
    "Training it on undirected graphs first let's us simplify the model\n",
    "so we will do the GAP-scalefree model on a E-R produced graph\n",
    "\"\"\"\n",
    "class GraphEmbeddingModule(nn.Module):\n",
    "    \"\"\"\n",
    "    This part of the network takes in a graph G = (V,E) and \n",
    "    produces node embeddings of each node in the network using \n",
    "    classical GraphSAGE layers.\n",
    "    We first implement a 4 layers of GraphSAGE with 128 hidden units.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                features,\n",
    "                feature_dim,\n",
    "                adj_lists,\n",
    "                embed_dim=64,\n",
    "                hidden_dim=128,\n",
    "                num_sample=2,\n",
    "                cuda=False):\n",
    "        \"\"\"\n",
    "        Note that the features as aggregated by the last layer are passed to the next\n",
    "        layer using a lambda function.\n",
    "        This means when we call the final layer, it will recursively call the\n",
    "        preceding layers.\n",
    "        \"\"\"\n",
    "        # TODO : This can be neatened up a lot!\n",
    "        super().__init__()\n",
    "        self.agg1 = MeanAggregator(features,cuda=cuda)\n",
    "        self.gSAGE1 = Encoder(features,\n",
    "                              feature_dim,\n",
    "                              hidden_dim,\n",
    "                              adj_lists,\n",
    "                              self.agg1,\n",
    "                              num_sample=num_sample,\n",
    "                              cuda=cuda)\n",
    "        self.agg2 = MeanAggregator(lambda nodes : self.gSAGE1(nodes).t(),\n",
    "                                   cuda=cuda,verbose=True)\n",
    "        self.gSAGE2 = Encoder(lambda nodes : self.gSAGE1(nodes).t(),\n",
    "                              hidden_dim,\n",
    "                              hidden_dim,\n",
    "                              adj_lists,\n",
    "                              self.agg2,\n",
    "                              num_sample=num_sample,\n",
    "                              cuda=cuda,\n",
    "                              base_model=self.gSAGE1,\n",
    "                             verbose=True)\n",
    "        self.agg3 = MeanAggregator(lambda nodes : self.gSAGE2(nodes).t(),cuda=cuda)\n",
    "        self.gSAGE3 = Encoder(lambda nodes : self.gSAGE2(nodes).t(),\n",
    "                              hidden_dim,\n",
    "                              hidden_dim,\n",
    "                              adj_lists,\n",
    "                              self.agg3,\n",
    "                              num_sample=num_sample,\n",
    "                              cuda=cuda,\n",
    "                             base_model=self.gSAGE2)\n",
    "        self.agg4 = MeanAggregator(lambda nodes : self.gSAGE3(nodes).t(),cuda=cuda)\n",
    "        self.gSAGE4 = Encoder(lambda nodes : self.gSAGE3(nodes).t(),\n",
    "                              hidden_dim,\n",
    "                              embed_dim,\n",
    "                              adj_lists,\n",
    "                              self.agg4,\n",
    "                              num_sample=num_sample,\n",
    "                              cuda=cuda,\n",
    "                             base_model=self.gSAGE3)\n",
    "        \n",
    "    def forward(self,n):\n",
    "       \"\"\"\n",
    "        Here we only need to call the last layer to call all!\n",
    "       \"\"\"\n",
    "       embeddings = self.gSAGE4(n)\n",
    "       return embeddings.t()\n",
    "\n",
    "    def change_graph(self,new_adj,new_features):\n",
    "       \"\"\"\n",
    "        Helper method to specialise to different graphs\n",
    "       \"\"\"\n",
    "       self.gSAGE1.adj_lists = new_adj\n",
    "       self.gSAGE2.adj_lists = new_adj\n",
    "       self.gSAGE3.adj_lists = new_adj\n",
    "       self.gSAGE4.adj_lists = new_adj\n",
    "\n",
    "       self.gSAGE1.features = new_features\n",
    "       self.agg1.features = new_features\n",
    "\n",
    "\n",
    "class GraphPartitioningModule(nn.Module):\n",
    "    \"\"\"\n",
    "    This part of the network takes in the node embeddings\n",
    "    and produces probabilities of the node belonging to each\n",
    "    state.\n",
    "    Roughly a transform from\n",
    "    [n_nodes, embedding_space_dim] --> [n_nodes,p_per_partition]\n",
    "    \"\"\"\n",
    "    def __init__(self,latent_dim,n_partitions):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(latent_dim,n_partitions)\n",
    "    \n",
    "    def forward(self,embeddings):\n",
    "        embeddings = self.dense(embeddings)\n",
    "        return F.softmax(embeddings,dim=1)\n",
    "\n",
    "                            \n",
    "\"\"\" Unit tests! \"\"\"\n",
    "if __name__ == '__main__':\n",
    "    adj, degrees, feats = featureless_random_graph(100)\n",
    "    adj_list = get_neigh_list(adj)\n",
    "    node_list = [*range(adj.shape[0])]\n",
    "    input_nodes = [*range(adj.shape[0])]\n",
    "    to_remove = []\n",
    "    for i in input_nodes:\n",
    "        if len(adj_list[i]) == 0:\n",
    "            print(f\"Disconnected Node {i}\")\n",
    "            to_remove.append(i)\n",
    "    for i in to_remove[::-1]:\n",
    "        input_nodes.pop(i)\n",
    "    features = get_feature_func(node_list,feats)\n",
    "    test_E = GraphEmbeddingModule(features,\n",
    "                      feats.shape[1],\n",
    "                      adj_list,\n",
    "                      cuda=False)\n",
    "    ## Let's embed a whole graph as a test\n",
    "    x = test_E(input_nodes)\n",
    "\n",
    "    # And feed this to a Graph Partition module\n",
    "    n_categories = 5\n",
    "    latent = 5\n",
    "    test_G = GraphPartitioningModule(latent,n_categories)\n",
    "    prob = test_G(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ff519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import networkx as nx\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from modules import GraphEmbeddingModule, GraphPartitioningModule\n",
    "from utils import *\n",
    "from datetime import datetime\n",
    "class GAPmodel(nn.Module):\n",
    "    \"\"\"\n",
    "    The full model.\n",
    "    Simply links the two modules together\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                features,\n",
    "                feature_dim,\n",
    "                adj_lists,\n",
    "                n_partitions=2,\n",
    "                embed_dim=64,\n",
    "                hidden_dim=128,\n",
    "                num_sample=10,\n",
    "                cuda=True):\n",
    "        super().__init__()\n",
    "        self.embedding = GraphEmbeddingModule(\n",
    "                            features,\n",
    "                            feature_dim,\n",
    "                            adj_lists,\n",
    "                            embed_dim=embed_dim,\n",
    "                            hidden_dim=hidden_dim,\n",
    "                            num_sample=num_sample,\n",
    "                            cuda=cuda\n",
    "                        )\n",
    "        self.partitioning = GraphPartitioningModule(\n",
    "                            embed_dim,\n",
    "                            n_partitions\n",
    "                        )\n",
    "    def forward(self,nodes):\n",
    "        nodes = self.embedding(nodes)\n",
    "        probability = self.partitioning(nodes)\n",
    "        return probability\n",
    "\n",
    "def expected_cut_loss(probabilities,\n",
    "                      degree,\n",
    "                      adj_matrix):\n",
    "    if not isinstance(adj_matrix, torch.Tensor):\n",
    "        adj_matrix = torch.tensor([[*a] for a in adj_matrix])\n",
    "    gamma = probabilities.t() @ degree\n",
    "    yt = (1-probabilities).t()\n",
    "    loss = (probabilities.div(gamma.t())) \n",
    "    loss = loss @ yt\n",
    "    loss = torch.sum(loss * adj_matrix)\n",
    "    return loss\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    date = datetime.now()\n",
    "    run_name = date.strftime(\"%b_%d_%H_%M\") + \"_run\"\n",
    "    # Training variables\n",
    "    # TODO : Move into argparser\n",
    "    n_graphs = 1   # n_unique graphs\n",
    "    n_nodes = 100 # n_nodes in graph\n",
    "    n_times = 100   # n_times exposed to each graph\n",
    "    epoch_gen = tqdm(range(n_graphs))\n",
    "    # Gen the first graph\n",
    "    input_nodes, adj_list, features, degree, adj = gen_graph(n_nodes,run_name,0)\n",
    "    features_inp = features(0).shape[0]\n",
    "    # Init the model\n",
    "    model = GAPmodel(features,\n",
    "                      features_inp,\n",
    "                      adj_list,\n",
    "                      cuda=False)\n",
    "    # Assign the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(),lr=7.5e-6)\n",
    "    # Begin Training\n",
    "    model.train()\n",
    "    for i in epoch_gen:\n",
    "        if i != 0:\n",
    "            # Generate a new random graph\n",
    "            input_nodes, adj_list, features, degree, adj = gen_graph(n_nodes,\n",
    "                                                                     run_name,i)\n",
    "            # Assign the model new features and adjacency\n",
    "            model.embedding.change_graph(adj_list,features)\n",
    "        for k in range(n_times):\n",
    "            optimizer.zero_grad()\n",
    "            node_probs = model(input_nodes)\n",
    "            loss = expected_cut_loss(node_probs,degree[:,1],adj)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"LOSS FOR THIS: \",loss.item())\n",
    "    with torch.no_grad():\n",
    "        image_graph(adj,run_name,i,predictions=node_probs)\n",
    "        ec, balance = rate_preds(adj,node_probs)\n",
    "        print(\"EDGE CUT :\",ec,\"BALANCE :\",balance)\n",
    "        with open(\"./graphs/scores.txt\",\"a\") as f:\n",
    "            f.write(f\"{run_name} {n_graphs} {n_nodes} {n_times} {ec} {balance}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfdba2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency:  torch.Size([100, 100])\n",
      "Degrees:  torch.Size([100, 2])\n",
      "Features:  torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "#utils.py conversion\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "from networkx.generators import erdos_renyi_graph as er_graph\n",
    "from networkx.generators import scale_free_graph as sfg\n",
    "from networkx.linalg import adjacency_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "\n",
    "\"\"\"\n",
    "Here we will: \n",
    "- Preprocess generated graphs\n",
    "- Implement methods to measure results\n",
    "\"\"\"\n",
    "def featureless_random_graph(n_nodes,prob=0.1,\n",
    "                kind=\"er\",\n",
    "                feature_size=10):\n",
    "    \"\"\"\n",
    "    Generates a random graph and then performs pca\n",
    "    on it and stores this as a feature matrix.\n",
    "    Returns: \n",
    "      -->   adjacency matrix    [n_nodes,n_nodes]\n",
    "      -->   node degree matrix  [n_nodes,1]\n",
    "      -->   node feature matrix [n_nodes,feature_size] \n",
    "      in that order.\n",
    "    Kind is either sf: Scale Free\n",
    "                   er: Erdos-Renyi\n",
    "    Features size :int: is the dimension of PCA to return.\n",
    "    \"\"\"\n",
    "    # Generate graph\n",
    "    if kind == \"sf\":\n",
    "        G = sfg(n_nodes)\n",
    "    elif kind == \"er\":\n",
    "        G = er_graph(n_nodes, prob)\n",
    "    else:\n",
    "        print(f\"Kind: {kind} not understood\")\n",
    "        raise Exception\n",
    "\n",
    "    # Generate adjacency matrix\n",
    "    adjacency_m = adjacency_matrix(G) # This returns a SPARSE matrix!\n",
    "    adjacency_m = adjacency_m.toarray()\n",
    "    adjacency_m = torch.tensor(adjacency_m)\n",
    "\n",
    "    # Generate node degree matrix\n",
    "    node_degrees = [*G.degree()]\n",
    "    node_degrees = torch.tensor(node_degrees).float()\n",
    "\n",
    "    # Generate node features\n",
    "    pca = PCA(n_components=feature_size)\n",
    "    node_features = pca.fit_transform(adjacency_m)\n",
    "    node_features = torch.tensor(node_features)\n",
    "\n",
    "    return adjacency_m, node_degrees, node_features\n",
    "\n",
    "# Utils\n",
    "def get_feature_func(nodes,features):\n",
    "    \"\"\"\n",
    "    Helper function to allow the node to call initial features like it would a\n",
    "    layer output.\n",
    "    \"\"\"\n",
    "    def feature_func(node):\n",
    "        return features[node]\n",
    "    return feature_func\n",
    "\n",
    "def get_neigh_list(adj_m,to_set=True):\n",
    "    adj_list = []\n",
    "    for node in adj_m.tolist():\n",
    "        neigh_list = []\n",
    "        for neigh_id in range(len(node)):\n",
    "            if node[neigh_id] != 0:\n",
    "                if not to_set:\n",
    "                    for j in range(node[neigh_id]):\n",
    "                        neigh_list.append(neigh_id)\n",
    "                else:\n",
    "                    neigh_list.append(neigh_id)\n",
    "        adj_list.append(set(neigh_list))\n",
    "    return adj_list\n",
    "\n",
    "def image_graph(adjacency_matrix,run_name,i,predictions=None):\n",
    "    G = nx.Graph(adjacency_matrix.detach().numpy())\n",
    "    path = f\"./graphs/{run_name}/\"\n",
    "    if predictions is not None:\n",
    "        test_labels = gen_test_labels(predictions)\n",
    "    else: \n",
    "        test_labels = None\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    plt.figure(figsize=(12,12))\n",
    "    nx.draw(G,node_color=test_labels,with_labels=True)\n",
    "    plt.savefig(path+str(i)+\"-result.png\")\n",
    "    plt.close()\n",
    "\n",
    "def gen_graph(n_nodes,run_name,epoch,save=True):\n",
    "    # Generate a random graph\n",
    "    adj, degrees, feats = featureless_random_graph(n_nodes)\n",
    "    adj_list = get_neigh_list(adj)\n",
    "    node_list = [*range(adj.shape[0])]\n",
    "    input_nodes = [*range(adj.shape[0])]\n",
    "    features = get_feature_func(node_list,feats)\n",
    "    if save and run_name:\n",
    "        path = f\"./graphs/{run_name}/\"\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "        plt.figure(figsize=(12,12))\n",
    "        nx.draw(nx.Graph(adj.detach().numpy()),with_labels=True)\n",
    "        plt.savefig(path + str(epoch) + \".png\")\n",
    "        plt.close()\n",
    "    return input_nodes, adj_list, features, degrees, adj\n",
    "\n",
    "def gen_test_labels(probabilities):\n",
    "    test_labels = [torch.argmax(p).item() for p in probabilities]\n",
    "    colour = []\n",
    "    for p in test_labels:\n",
    "        if p == 0:\n",
    "            colour.append(\"blue\")\n",
    "        if p == 1:\n",
    "            colour.append(\"red\")\n",
    "        if p == 2:\n",
    "            colour.append(\"yellow\")\n",
    "        if p == 3:\n",
    "            colour.append(\"olive\")\n",
    "        if p == 4:\n",
    "            colour.append(\"cyan\")\n",
    "    return colour\n",
    "\n",
    "def rate_preds(adj,preds):\n",
    "    n_nodes = adj.shape[0]\n",
    "    # Take the predicted state\n",
    "    states = [max(p) for p in preds]\n",
    "    # Get edge-list\n",
    "    edges = adj.nonzero(as_tuple=False).numpy()\n",
    "    # Calc edge cut\n",
    "    edge_cut = calc_edge_cut(edges,states)\n",
    "    # Calc balance\n",
    "    balance = calc_balance(states)\n",
    "    return edge_cut, balance\n",
    "\n",
    "def calc_edge_cut(edges,states):\n",
    "    \"\"\"\n",
    "    Edge cut is the ratio of the cut to the number of edges.\n",
    "    edges should be a iterable of iterables of edge indices.\n",
    "    states should be a list of the predicted states\n",
    "    \"\"\"\n",
    "    n_edges = len(edges)\n",
    "    states = {i:s for i,s in enumerate(states)}\n",
    "    cuts = 0\n",
    "    for (state1,state2) in edges:\n",
    "        if states[state1] != states[state2]:\n",
    "            cuts += 1\n",
    "    return cuts / n_edges\n",
    "\n",
    "\n",
    "def calc_balance(states):\n",
    "    \"\"\"\n",
    "    From what I can gather:\n",
    "    B = 1 - MSE(node_populations,n_nodes/n_parts)\n",
    "    B = 1 - 1/n*sum(node_pop - n_nodes/n_parts)**2\n",
    "    \"\"\"\n",
    "    n_nodes = len(states)\n",
    "    node_pops = Counter(states)\n",
    "    balance = n_nodes / len(node_pops.keys())\n",
    "    mse = sum((v - balance)**2 for v in node_pops.values()) \n",
    "    mse *= 1 / n_nodes\n",
    "    return 1 - mse\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    Unit tests!\n",
    "    \"\"\"\n",
    "    adj,degrees,features = featureless_random_graph(100)\n",
    "    print(\"Adjacency: \", adj.shape)\n",
    "    print(\"Degrees: \" ,  degrees.shape)\n",
    "    print(\"Features: \",  features.shape)\n",
    "\n",
    "    # Getting into right format for layers\n",
    "    adj_list = get_neigh_list(adj)\n",
    "    node_list = [*range(adj.shape[0])]\n",
    "    features = get_feature_func(node_list,features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16caaf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{2, 5, 6, 10, 15, 16, 21}, {3, 5, 7, 12, 13, 15, 19, 20, 21, 23}, {0, 4, 7, 13, 14, 16, 18, 19, 20, 21, 22, 23}, {1, 6, 8, 9, 11, 15, 22, 24}, {2, 5, 6, 7, 8, 9, 10, 11, 17, 18, 19, 22, 61}, {0, 1, 4, 8, 10, 13, 18, 19, 20}, {0, 3, 4, 10, 12, 13, 14, 17, 19, 20, 29}, {1, 2, 4, 9, 16, 17, 18, 19, 21, 22, 23}, {3, 4, 5, 15, 18, 19, 21, 24}, {3, 4, 7, 10, 13, 14, 15, 16, 17, 21, 23, 24}, {0, 4, 5, 6, 9, 14, 17, 19, 21, 22, 23, 24}, {3, 4, 12, 14, 17, 19}, {1, 6, 11, 15, 16, 17, 19, 24}, {1, 2, 5, 6, 9, 14, 15, 16, 17, 18, 20, 22, 23, 24}, {2, 6, 9, 10, 11, 13, 15, 17, 23}, {0, 1, 3, 8, 9, 12, 13, 14, 16, 17, 82, 24}, {0, 2, 7, 9, 12, 13, 15, 50, 52, 23, 24}, {34, 4, 6, 7, 9, 10, 11, 12, 13, 14, 15, 18, 19, 21, 23, 24}, {2, 4, 5, 7, 8, 13, 17, 19, 22}, {1, 2, 4, 5, 6, 7, 8, 10, 11, 12, 17, 18, 22}, {1, 2, 5, 6, 13, 45, 21}, {0, 1, 2, 7, 8, 9, 10, 17, 20, 22}, {2, 3, 4, 7, 10, 13, 79, 18, 19, 21, 26}, {1, 2, 7, 9, 10, 13, 14, 16, 17, 24}, {3, 8, 9, 10, 12, 13, 15, 16, 17, 23}, {33, 34, 35, 39, 43, 44, 48, 49, 29}, {33, 36, 37, 38, 41, 44, 45, 46, 48, 49, 22, 27, 29, 31}, {32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 44, 45, 46, 47, 26, 31}, {32, 34, 35, 36, 37, 38, 40, 41, 42, 44, 46, 48, 85, 31}, {33, 38, 6, 40, 47, 48, 49, 25, 26}, {34, 35, 36, 37, 40, 41, 43, 44, 46, 48, 49}, {32, 35, 37, 38, 39, 41, 42, 43, 44, 45, 47, 49, 26, 27, 28}, {33, 34, 35, 38, 39, 44, 47, 87, 27, 28, 31}, {32, 34, 35, 38, 39, 40, 41, 25, 26, 27, 29}, {32, 33, 35, 36, 68, 41, 47, 48, 17, 49, 84, 25, 27, 28, 30}, {32, 33, 34, 36, 40, 41, 42, 45, 48, 25, 27, 28, 30, 31}, {34, 35, 40, 42, 46, 26, 27, 28, 30}, {38, 40, 42, 46, 47, 49, 82, 26, 28, 30, 31}, {32, 33, 37, 70, 40, 43, 48, 49, 26, 27, 28, 29, 31}, {32, 33, 40, 41, 42, 46, 49, 25, 27, 60, 31}, {33, 35, 36, 37, 38, 39, 41, 42, 43, 44, 45, 27, 28, 29, 30}, {33, 34, 35, 39, 40, 42, 48, 26, 27, 28, 30, 31}, {35, 36, 37, 39, 40, 41, 43, 45, 46, 47, 79, 27, 28, 31}, {38, 40, 42, 47, 48, 25, 30, 31}, {32, 40, 45, 49, 25, 26, 27, 28, 30, 31}, {35, 40, 42, 44, 48, 49, 20, 26, 27, 31}, {36, 37, 39, 59, 42, 26, 27, 28, 30}, {32, 34, 37, 42, 43, 27, 29, 31}, {34, 35, 38, 41, 43, 45, 49, 25, 26, 28, 29, 30}, {34, 37, 38, 39, 44, 45, 48, 25, 26, 29, 30, 31}, {64, 66, 67, 68, 69, 70, 16, 55, 59, 61}, {64, 66, 68, 69, 70, 72, 73, 84, 53, 55, 62}, {66, 67, 68, 69, 70, 72, 74, 16, 53, 55, 56, 92, 61, 63}, {64, 67, 68, 72, 73, 74, 51, 52, 54, 57, 59, 60, 61}, {64, 66, 69, 70, 71, 73, 74, 53, 55, 56, 57, 91, 60, 63}, {64, 66, 68, 70, 73, 74, 50, 51, 52, 54, 56, 57, 60, 62}, {65, 69, 70, 71, 74, 52, 54, 55, 57, 61}, {67, 68, 69, 70, 71, 74, 53, 54, 55, 56, 60, 62, 63}, {67, 70, 74, 59, 60, 61}, {65, 66, 70, 71, 73, 46, 50, 82, 53, 58, 61}, {64, 65, 66, 67, 68, 69, 39, 73, 93, 53, 54, 55, 87, 57, 58, 61}, {96, 65, 66, 4, 68, 50, 52, 53, 56, 58, 59, 60}, {64, 66, 67, 69, 70, 71, 72, 73, 74, 51, 55, 57, 63}, {69, 77, 52, 54, 57, 62}, {65, 66, 67, 68, 69, 72, 73, 74, 50, 51, 53, 54, 55, 60, 62}, {64, 68, 69, 70, 71, 73, 56, 59, 60, 61}, {64, 71, 72, 73, 50, 51, 52, 54, 55, 59, 60, 61, 62}, {64, 68, 70, 72, 73, 74, 50, 52, 53, 57, 58, 60, 62}, {64, 65, 34, 67, 73, 74, 50, 51, 52, 53, 55, 57, 60, 61}, {64, 65, 72, 73, 50, 51, 52, 54, 56, 57, 60, 62, 63}, {65, 67, 38, 71, 72, 74, 80, 50, 51, 52, 54, 55, 56, 57, 58, 59, 62}, {65, 66, 70, 72, 74, 54, 56, 57, 59, 62}, {64, 66, 67, 69, 70, 71, 74, 51, 52, 53, 62}, {64, 65, 66, 67, 68, 69, 74, 51, 53, 54, 55, 59, 60, 62}, {64, 89, 67, 68, 70, 71, 72, 73, 52, 53, 54, 55, 56, 57, 58, 62}, {96, 98, 78, 81, 82, 85, 86, 89, 92, 93, 94}, {98, 78, 80, 86, 90, 91, 92, 93, 94}, {98, 99, 81, 83, 84, 86, 87, 88, 95, 63}, {75, 76, 79, 81, 83, 86, 89, 90, 92, 95}, {98, 42, 78, 81, 82, 83, 84, 85, 22, 90, 92, 94}, {97, 98, 70, 76, 81, 84, 86, 87, 88, 89, 90, 92, 94}, {96, 75, 77, 78, 79, 80, 84, 86, 88, 89, 92, 93, 95}, {99, 37, 91, 75, 79, 15, 88, 90, 59, 92, 94, 95}, {96, 98, 99, 77, 78, 79, 84, 89, 91, 94, 95}, {97, 34, 99, 77, 79, 80, 81, 51, 83, 85, 86, 88, 90, 93}, {99, 75, 79, 92, 84, 86, 87, 88, 89, 90, 91, 28, 95}, {98, 99, 75, 76, 77, 78, 80, 81, 84, 85, 89, 90, 94}, {32, 97, 77, 92, 80, 85, 88, 89, 60, 93, 95}, {96, 77, 80, 81, 82, 84, 85, 87, 93}, {97, 74, 75, 78, 80, 81, 83, 85, 86, 87, 90, 91, 94}, {76, 78, 79, 80, 82, 84, 85, 86, 89, 92, 94, 95}, {96, 97, 98, 76, 82, 83, 85, 54, 89, 92, 94}, {97, 98, 75, 76, 78, 79, 80, 81, 82, 52, 85, 87, 90, 91, 93}, {97, 99, 75, 76, 92, 81, 84, 87, 88, 60, 94}, {96, 98, 99, 75, 76, 79, 80, 82, 83, 86, 89, 90, 91, 93, 95}, {96, 97, 77, 78, 81, 82, 83, 85, 87, 90, 94}, {97, 98, 99, 75, 81, 83, 88, 91, 61, 94, 95}, {96, 80, 84, 87, 89, 91, 92, 93, 95}, {96, 99, 75, 76, 77, 79, 80, 83, 86, 91, 92, 94}, {96, 98, 77, 82, 83, 84, 85, 86, 93, 94}]\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import math\n",
    "from os.path import dirname, join as pjoin\n",
    "import scipy.io as sio\n",
    "from matplotlib import pyplot as plt\n",
    "from networkx.generators import erdos_renyi_graph as er_graph\n",
    "from networkx.generators import scale_free_graph as sfg\n",
    "from networkx.linalg import adjacency_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "#ADJ MATRIX FROM MATLAB\n",
    "\n",
    "Adj = np.loadtxt(r'M:\\Masters\\Dynamical Coarse Graining in Complex Networks\\Adj.txt')\n",
    "\n",
    "def get_neigh_list(adj_m,to_set=True):\n",
    "    adj_list = []\n",
    "    for node in adj_m.tolist():\n",
    "        neigh_list = []\n",
    "        for neigh_id in range(len(node)):\n",
    "            if node[neigh_id] != 0:\n",
    "                if not to_set:\n",
    "                    for j in range(node[neigh_id]):\n",
    "                        neigh_list.append(neigh_id)\n",
    "                else:\n",
    "                    neigh_list.append(neigh_id)\n",
    "        adj_list.append(set(neigh_list))\n",
    "    return adj_list\n",
    "\n",
    "def image_graph(adjacency_matrix,run_name,i,predictions=None):\n",
    "    G = nx.Graph(adjacency_matrix.detach().numpy())\n",
    "    path = f\"./graphs/{run_name}/\"\n",
    "    if predictions is not None:\n",
    "        test_labels = gen_test_labels(predictions)\n",
    "    else: \n",
    "        test_labels = None\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    plt.figure(figsize=(12,12))\n",
    "    nx.draw(G,node_color=test_labels,with_labels=True)\n",
    "    plt.savefig(path+str(i)+\"-result.png\")\n",
    "    plt.close()\n",
    "\n",
    "adj_list = get_neigh_list(Adj)\n",
    "print(adj_list)\n",
    "\n",
    "Adj = torch.tensor(Adj)\n",
    "image_graph(Adj, 1, 100)\n",
    "print(Adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38b32708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 1, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32), tensor([[ 0.,  5.],\n",
      "        [ 1., 12.],\n",
      "        [ 2., 12.],\n",
      "        [ 3., 12.],\n",
      "        [ 4., 13.],\n",
      "        [ 5., 16.],\n",
      "        [ 6., 12.],\n",
      "        [ 7., 11.],\n",
      "        [ 8., 10.],\n",
      "        [ 9.,  7.],\n",
      "        [10.,  9.],\n",
      "        [11.,  6.],\n",
      "        [12., 10.],\n",
      "        [13., 12.],\n",
      "        [14.,  7.],\n",
      "        [15., 13.],\n",
      "        [16.,  5.],\n",
      "        [17., 15.],\n",
      "        [18., 10.],\n",
      "        [19.,  5.],\n",
      "        [20., 11.],\n",
      "        [21., 12.],\n",
      "        [22., 13.],\n",
      "        [23., 11.],\n",
      "        [24., 11.],\n",
      "        [25., 10.],\n",
      "        [26.,  7.],\n",
      "        [27., 19.],\n",
      "        [28.,  8.],\n",
      "        [29., 11.],\n",
      "        [30.,  4.],\n",
      "        [31.,  8.],\n",
      "        [32.,  8.],\n",
      "        [33., 10.],\n",
      "        [34., 10.],\n",
      "        [35., 10.],\n",
      "        [36., 14.],\n",
      "        [37.,  8.],\n",
      "        [38.,  9.],\n",
      "        [39., 13.],\n",
      "        [40., 10.],\n",
      "        [41., 11.],\n",
      "        [42., 11.],\n",
      "        [43.,  6.],\n",
      "        [44.,  6.],\n",
      "        [45.,  9.],\n",
      "        [46., 14.],\n",
      "        [47., 13.],\n",
      "        [48., 19.],\n",
      "        [49., 10.],\n",
      "        [50., 11.],\n",
      "        [51.,  7.],\n",
      "        [52., 14.],\n",
      "        [53.,  6.],\n",
      "        [54., 10.],\n",
      "        [55.,  7.],\n",
      "        [56.,  5.],\n",
      "        [57.,  8.],\n",
      "        [58., 11.],\n",
      "        [59.,  7.],\n",
      "        [60.,  5.],\n",
      "        [61., 11.],\n",
      "        [62.,  6.],\n",
      "        [63., 15.],\n",
      "        [64.,  8.],\n",
      "        [65.,  8.],\n",
      "        [66.,  9.],\n",
      "        [67., 11.],\n",
      "        [68.,  8.],\n",
      "        [69.,  9.],\n",
      "        [70., 11.],\n",
      "        [71.,  8.],\n",
      "        [72.,  5.],\n",
      "        [73.,  7.],\n",
      "        [74., 12.],\n",
      "        [75.,  8.],\n",
      "        [76.,  9.],\n",
      "        [77.,  6.],\n",
      "        [78.,  9.],\n",
      "        [79., 11.],\n",
      "        [80.,  7.],\n",
      "        [81., 12.],\n",
      "        [82.,  8.],\n",
      "        [83.,  5.],\n",
      "        [84., 12.],\n",
      "        [85., 11.],\n",
      "        [86., 12.],\n",
      "        [87., 15.],\n",
      "        [88.,  5.],\n",
      "        [89., 10.],\n",
      "        [90.,  6.],\n",
      "        [91.,  7.],\n",
      "        [92., 11.],\n",
      "        [93.,  9.],\n",
      "        [94.,  6.],\n",
      "        [95., 10.],\n",
      "        [96.,  6.],\n",
      "        [97.,  9.],\n",
      "        [98., 17.],\n",
      "        [99.,  8.]]), tensor([[-2.6097e-01, -1.4216e-01, -2.7622e-01,  7.5332e-02, -5.0968e-01,\n",
      "          2.0774e-02, -1.2215e-01,  3.8672e-01, -2.8606e-01, -2.6679e-01],\n",
      "        [-7.3440e-02, -1.0856e+00, -1.5856e-01,  1.1274e+00,  4.3046e-01,\n",
      "          2.1635e-01,  4.5407e-01, -6.7125e-01,  3.8366e-01, -8.8942e-01],\n",
      "        [ 1.3295e+00,  1.4865e-01, -7.8247e-01, -7.6037e-01,  3.4779e-01,\n",
      "         -3.3566e-01,  3.3631e-01,  6.6597e-01,  6.6708e-01, -1.7739e-01],\n",
      "        [-4.4491e-01,  7.6460e-01, -1.0085e+00, -2.0388e-01,  2.2827e-01,\n",
      "         -1.2242e+00, -1.7737e-01,  3.1026e-01, -3.0414e-01,  2.5314e-01],\n",
      "        [ 1.4620e+00, -6.4599e-01, -2.9512e-01, -3.5536e-01,  2.4082e-01,\n",
      "          6.9403e-01,  1.1129e+00,  1.0276e-01,  1.4423e-01, -6.4304e-01],\n",
      "        [-9.9608e-01, -7.8130e-01, -2.0772e-01,  1.7300e+00,  1.0673e+00,\n",
      "         -5.9661e-02, -3.3131e-01,  2.0243e+00, -6.2408e-02,  1.0173e-01],\n",
      "        [-1.1087e+00, -9.5915e-02,  2.0981e-01,  7.1006e-01, -3.9018e-01,\n",
      "          6.2935e-01, -4.4192e-01,  1.2424e+00,  3.2884e-01, -1.1274e-01],\n",
      "        [ 7.6164e-01, -4.8083e-01,  3.0918e-01, -5.2097e-01, -7.5631e-01,\n",
      "          1.5665e-01, -8.1138e-01, -1.6476e-01,  1.7078e-01,  1.8395e-01],\n",
      "        [ 4.9581e-02, -6.3976e-01,  1.3467e+00, -1.3010e-01,  4.6485e-01,\n",
      "         -4.0327e-01,  8.8900e-01, -7.1489e-02, -4.7715e-01, -3.6282e-01],\n",
      "        [ 9.1412e-02, -1.4825e-01, -1.4825e-01, -2.2990e-01,  1.9035e-01,\n",
      "         -4.2261e-01, -2.6192e-01, -1.8934e-01,  5.5044e-02,  8.0664e-01],\n",
      "        [ 3.2581e-01, -1.0287e-02,  4.7781e-01, -6.7081e-01,  2.8711e-01,\n",
      "         -7.1757e-01,  1.0777e-01, -4.3211e-01, -5.0719e-01, -6.5878e-01],\n",
      "        [ 4.6983e-01, -2.4045e-02, -5.8062e-01,  3.6217e-02, -1.8399e-02,\n",
      "          5.9603e-01,  5.2626e-01, -3.6280e-01,  4.5100e-01, -1.8984e-01],\n",
      "        [ 7.5169e-01, -1.9360e-01, -1.8513e-01, -1.5160e-01, -6.8455e-01,\n",
      "         -2.9243e-01,  9.3023e-02,  2.0156e-01, -9.4388e-02,  2.7512e-01],\n",
      "        [ 1.2425e+00, -8.8909e-02,  5.1651e-02,  4.2491e-01, -6.8816e-01,\n",
      "          4.0709e-02, -5.2582e-01, -6.6554e-02, -8.6188e-01, -1.1967e-01],\n",
      "        [-3.5657e-03, -6.4857e-02, -9.1708e-01,  6.9806e-01, -7.1667e-01,\n",
      "         -3.4160e-01,  2.2127e-01,  1.4443e-01,  6.9846e-02, -9.2108e-02],\n",
      "        [-9.2461e-01,  9.5412e-02,  3.6207e-01, -9.0643e-02,  5.5473e-01,\n",
      "          1.7498e-01,  1.4876e+00, -4.3578e-01,  2.2993e-01, -6.0030e-02],\n",
      "        [-1.3250e-01,  3.4446e-02,  4.6269e-01,  2.6463e-01, -4.2095e-01,\n",
      "          9.3472e-02,  3.4192e-01,  1.1239e-01,  2.6553e-02,  5.1979e-02],\n",
      "        [-1.2241e-03,  1.0444e+00, -1.2771e+00,  6.2899e-01,  2.8346e-01,\n",
      "         -1.8201e-01,  5.8896e-01, -2.0833e-01,  1.4645e+00, -2.1898e-01],\n",
      "        [-2.9828e-02, -7.3360e-01, -5.5037e-03,  2.6612e-01,  1.3794e-01,\n",
      "         -2.8680e-01, -1.4307e+00,  1.7362e-01,  3.6351e-01,  7.2976e-01],\n",
      "        [ 3.7202e-01, -3.0712e-01,  9.4088e-02,  1.0980e-02, -2.0371e-01,\n",
      "          2.8891e-01, -7.3830e-01, -1.1113e-01, -5.2773e-01, -5.3265e-02],\n",
      "        [ 4.7311e-01,  7.4498e-01, -4.9980e-01,  1.5286e-01,  5.0708e-01,\n",
      "         -1.5367e-01,  8.1629e-01, -5.3192e-01,  5.9237e-02, -8.4955e-02],\n",
      "        [ 3.0307e-01, -5.6393e-01,  2.8938e-01, -7.8901e-01,  1.1529e+00,\n",
      "         -1.0196e+00,  1.2071e-01,  2.0337e-01,  6.6842e-02,  4.6700e-01],\n",
      "        [ 1.3303e+00,  2.3592e-01,  1.0642e+00,  5.8194e-01,  3.8499e-01,\n",
      "          2.3323e-01,  1.1193e-01,  1.1897e+00, -7.2392e-01, -1.7368e-01],\n",
      "        [ 3.1371e-01, -7.7838e-01, -1.9499e-01, -3.5541e-01,  5.0230e-01,\n",
      "         -7.2141e-01, -4.9995e-01,  2.3659e-01, -2.7064e-01, -1.0166e+00],\n",
      "        [-8.7529e-01, -6.7593e-01, -3.8359e-01,  8.2595e-01, -6.6541e-01,\n",
      "         -1.9790e-01,  8.0422e-01, -6.6991e-02,  8.2663e-02,  8.4692e-01],\n",
      "        [ 8.5471e-01,  8.0553e-01, -4.1176e-01,  4.7971e-01, -8.0131e-01,\n",
      "          1.3164e-02,  7.4734e-01,  4.0718e-01,  5.3633e-01,  8.1001e-02],\n",
      "        [ 3.9537e-01,  3.4848e-01, -2.5622e-01, -3.7816e-01, -7.8483e-01,\n",
      "          1.1842e-01, -2.6172e-01,  1.2262e-02,  5.1308e-01, -1.0998e-01],\n",
      "        [-1.1204e+00,  1.2242e+00,  1.4065e+00, -8.7353e-01, -1.1044e+00,\n",
      "         -1.0804e+00,  3.7242e-01,  1.0292e-01,  5.8017e-01, -1.1038e-01],\n",
      "        [-6.0462e-01, -2.5552e-01, -1.5696e-01, -3.2053e-01, -2.7312e-01,\n",
      "          7.6263e-01,  3.7061e-01, -1.9426e-01, -6.5981e-01,  6.3971e-01],\n",
      "        [-4.4939e-01, -7.4729e-01,  3.7004e-01,  1.8715e-01,  4.2255e-02,\n",
      "         -3.2524e-01,  2.7183e-02, -4.4214e-01,  2.5175e-01,  9.2337e-01],\n",
      "        [ 7.2593e-02,  3.3385e-01, -5.6805e-01,  6.2077e-02, -9.5780e-03,\n",
      "         -7.5889e-02,  1.5587e-01, -2.2159e-01,  2.2626e-01, -2.6953e-01],\n",
      "        [-2.1946e-01,  1.4131e-01, -2.4216e-01, -3.7913e-01, -5.2225e-01,\n",
      "         -3.1828e-01, -3.2189e-01, -4.9119e-02, -1.1765e-01, -6.4727e-01],\n",
      "        [-2.4995e-01, -8.5088e-02,  3.2298e-01,  3.4070e-02, -3.3481e-01,\n",
      "         -1.0214e-01, -8.4220e-01, -2.2306e-01,  2.2022e-01, -4.3320e-02],\n",
      "        [ 4.2727e-01,  6.0608e-01, -1.4407e-01, -3.6962e-01, -2.8263e-01,\n",
      "         -6.4853e-01,  3.5336e-01,  1.3688e-01, -1.7128e-01,  6.5958e-01],\n",
      "        [ 4.4844e-01,  9.9077e-01, -3.8203e-01,  7.7542e-01, -4.5682e-01,\n",
      "          3.2383e-01, -2.1776e-01,  1.0982e-01, -6.5339e-02, -8.1181e-01],\n",
      "        [ 6.9617e-01,  1.1627e+00,  5.7782e-01,  7.8739e-02,  1.3674e-01,\n",
      "          7.3993e-02,  4.2189e-01, -3.4885e-01,  5.5975e-01,  1.6696e-01],\n",
      "        [-1.5132e+00, -4.7483e-01,  1.9913e-01, -4.2253e-01,  8.3398e-01,\n",
      "          1.0281e-01,  7.2943e-01,  6.1530e-01, -2.2796e-01, -3.7154e-01],\n",
      "        [ 1.1220e-02, -4.0115e-01, -2.8596e-01,  1.8133e-01,  9.0070e-02,\n",
      "          5.9304e-01,  7.0187e-01, -7.9581e-01,  1.2535e-01, -1.2775e-01],\n",
      "        [-3.0454e-01,  3.6121e-01, -4.2630e-01, -6.9510e-01,  4.9391e-01,\n",
      "          1.7815e-01, -4.2409e-01, -2.8771e-01, -9.9201e-01, -8.0248e-01],\n",
      "        [ 8.2376e-01, -1.0541e+00, -1.2662e-03, -8.8553e-01, -1.5432e-01,\n",
      "         -3.9290e-01,  5.1191e-01,  8.7609e-01,  6.2533e-01, -1.5699e-01],\n",
      "        [-1.1859e+00, -5.3936e-01, -3.3255e-01,  1.8299e-01, -4.9655e-01,\n",
      "          2.8404e-01,  1.7561e-02,  1.7341e-02, -6.8172e-01,  5.6731e-01],\n",
      "        [-1.7046e-01, -8.7815e-01,  1.1347e+00, -1.5669e-01,  5.7326e-01,\n",
      "         -4.3945e-01,  1.8617e-02, -3.8322e-01,  6.7805e-01,  8.4467e-02],\n",
      "        [ 8.3160e-01, -6.8175e-01, -1.1587e+00,  4.3896e-01,  4.1103e-01,\n",
      "          2.9041e-01, -5.3205e-01,  3.3293e-01,  7.3620e-01, -1.6893e-01],\n",
      "        [ 1.3113e-03, -8.2205e-04, -3.5241e-02, -3.2352e-01, -3.7541e-01,\n",
      "          6.8149e-01, -3.3546e-01, -3.6233e-01,  4.3041e-01, -2.7411e-02],\n",
      "        [-3.8998e-01, -1.0308e-01,  5.7228e-01, -2.6289e-01, -4.6132e-01,\n",
      "          2.9761e-01, -1.4983e-01, -2.1930e-01,  3.8936e-01,  1.7539e-01],\n",
      "        [-5.2351e-01,  5.0904e-01, -1.9460e-01,  1.1134e-01, -4.0188e-01,\n",
      "          3.6455e-01,  4.2276e-01,  4.2167e-01, -3.7393e-01,  4.0654e-01],\n",
      "        [-5.0935e-01,  6.2739e-01, -9.0976e-01, -1.3719e+00,  1.1117e+00,\n",
      "          4.6682e-01,  1.2317e-01,  1.3354e+00, -7.8132e-01,  1.7311e-01],\n",
      "        [ 1.6889e+00,  1.0276e-01,  5.8739e-01,  7.4605e-05,  6.7215e-01,\n",
      "          8.9015e-01, -4.1657e-01,  3.7154e-01, -8.6041e-01, -1.2902e-02],\n",
      "        [ 1.5276e+00,  1.5942e+00,  1.5150e-01,  1.0923e+00,  1.5390e+00,\n",
      "          2.1201e-01, -1.0499e-01,  2.4641e-01,  5.9656e-01,  1.0432e+00],\n",
      "        [-1.9158e-01,  3.0272e-01, -3.3257e-01, -6.5788e-01, -1.8660e-01,\n",
      "          7.2980e-01,  4.9368e-01, -8.1922e-02,  2.5541e-02,  1.2087e+00],\n",
      "        [ 6.9293e-01,  2.1290e-01,  4.2682e-01, -3.1568e-01, -2.8866e-01,\n",
      "          1.2015e-01, -3.3161e-01,  4.4576e-02,  3.5919e-01, -4.1485e-02],\n",
      "        [-6.3544e-01,  5.1473e-01, -1.6831e-01, -3.8780e-01, -1.1676e-01,\n",
      "          4.2515e-01, -3.0433e-01, -2.1549e-01, -6.2329e-01,  1.7198e-01],\n",
      "        [ 7.1731e-01, -9.5440e-01,  1.1496e+00, -1.4576e+00, -1.4316e-01,\n",
      "          3.9527e-01,  1.5662e-01,  7.4037e-01,  8.2095e-01,  4.5012e-01],\n",
      "        [ 2.6302e-01,  5.4490e-01, -4.6585e-01,  2.5108e-01, -3.8893e-01,\n",
      "         -3.1761e-01, -3.6772e-01,  1.6077e-01, -1.0160e-01,  3.5541e-01],\n",
      "        [ 4.2296e-01,  4.8073e-01,  1.6943e-01,  1.3929e-01, -9.7559e-02,\n",
      "          1.0657e+00, -5.5913e-01, -2.4125e-01, -7.9442e-01, -7.4566e-01],\n",
      "        [ 5.3675e-01,  5.0895e-01, -1.6135e-02,  2.2160e-01,  1.4790e-02,\n",
      "          5.4956e-01,  7.2243e-01, -6.6976e-01,  4.5731e-01, -2.5451e-03],\n",
      "        [-7.0297e-01,  2.9762e-01, -2.9826e-01, -3.9102e-01,  3.7066e-01,\n",
      "          8.7632e-02,  1.6198e-01, -2.6846e-01, -6.8233e-01, -5.9098e-03],\n",
      "        [ 3.2927e-01, -1.0240e-01, -4.2659e-01,  1.8840e-01, -8.9417e-01,\n",
      "         -7.4893e-02, -2.9860e-01,  2.0711e-01,  1.5736e-01, -3.8608e-01],\n",
      "        [-5.5177e-01, -2.4951e-02, -3.9315e-01,  5.3278e-04, -4.4275e-01,\n",
      "          4.3789e-02,  7.1885e-01,  2.7048e-01, -6.4345e-01,  9.3500e-01],\n",
      "        [ 8.2484e-02, -9.0697e-02, -2.1780e-02, -3.2346e-01, -6.9721e-01,\n",
      "          5.3901e-02, -2.8892e-01, -3.2614e-01, -3.7042e-01, -4.6855e-02],\n",
      "        [-4.1708e-01, -4.3730e-01,  3.1746e-01,  2.8190e-01, -4.5867e-01,\n",
      "          1.0941e-01, -5.7683e-02,  6.0365e-02,  1.3200e-01,  1.0988e-01],\n",
      "        [ 3.1026e-01,  6.8310e-01,  1.9083e-01, -8.4475e-01, -8.7265e-01,\n",
      "         -9.4065e-03,  4.5199e-01,  9.0159e-01,  3.7596e-02,  2.8347e-02],\n",
      "        [ 8.9105e-02,  2.4832e-02, -5.2067e-01,  3.4666e-01, -4.6623e-01,\n",
      "         -3.8608e-01, -5.9414e-01, -1.7809e-01,  2.1492e-01, -3.1311e-01],\n",
      "        [ 3.5360e-01,  6.0856e-01,  1.6311e+00,  1.3971e+00,  7.3892e-01,\n",
      "         -8.8009e-01, -5.0871e-04, -2.2463e-01, -3.7112e-01,  7.5347e-01],\n",
      "        [-1.3488e-01, -4.9336e-01, -2.4515e-01, -3.8407e-01,  2.9470e-01,\n",
      "         -8.0959e-01,  3.8252e-01, -7.7910e-01, -2.5241e-01, -8.0564e-01],\n",
      "        [ 9.9607e-02, -4.6009e-01, -3.6099e-01,  1.7622e-01,  7.3101e-01,\n",
      "         -5.6702e-01, -9.4987e-01, -4.2379e-01, -2.4687e-01,  3.1889e-01],\n",
      "        [-1.9557e-01, -8.8494e-01,  3.9412e-01,  3.5429e-01,  4.0656e-01,\n",
      "         -1.0833e+00,  2.9392e-01, -6.1340e-01,  4.0077e-02, -6.2336e-01],\n",
      "        [-2.2684e-03, -1.3824e-01, -5.2715e-02,  2.0113e-01,  2.6829e-01,\n",
      "          1.0596e+00,  5.9009e-02, -5.0083e-01, -8.8826e-02,  7.3655e-01],\n",
      "        [-1.3968e-01,  9.0755e-01,  3.7082e-01,  1.0777e-01,  6.4127e-01,\n",
      "         -7.8490e-01, -3.3124e-02, -3.4070e-01, -2.7526e-01, -1.6641e-01],\n",
      "        [-4.3477e-01,  7.0624e-02, -6.9542e-01, -5.9356e-01,  7.9685e-01,\n",
      "          1.5505e-01,  3.6972e-01, -1.1245e-01,  1.4017e-01, -4.1678e-01],\n",
      "        [-2.3419e-01,  2.7350e-01,  1.6043e-01,  6.5370e-01, -7.6750e-01,\n",
      "          1.2967e-01,  1.5575e-01,  1.0865e+00, -1.8960e-01, -5.3931e-01],\n",
      "        [-4.5106e-01,  6.6395e-01,  4.4769e-01, -1.5102e-01,  3.1966e-01,\n",
      "          1.6876e-01, -7.4668e-01, -3.8539e-01,  4.2188e-01, -3.7494e-01],\n",
      "        [-2.8534e-01, -2.6651e-01, -1.8549e-01, -3.1055e-02, -5.1898e-03,\n",
      "          4.0464e-01, -2.3893e-01, -5.0650e-01, -2.4467e-02, -6.5443e-01],\n",
      "        [-5.0573e-01, -4.5066e-02,  5.0750e-01,  5.9780e-01, -4.9375e-01,\n",
      "         -4.9272e-01,  1.4344e-02,  5.2841e-02, -2.2237e-02, -1.4911e-01],\n",
      "        [ 3.9449e-02,  2.2506e-01,  3.3809e-01,  5.4918e-01,  7.4782e-01,\n",
      "          5.2798e-01,  7.6322e-01, -3.8348e-01, -9.1811e-01,  5.9976e-01],\n",
      "        [-3.6335e-01, -4.9728e-01, -3.7828e-01, -3.7608e-01, -3.0271e-01,\n",
      "         -1.1697e-02,  6.7631e-01, -5.9105e-02, -3.5082e-01,  1.1572e-01],\n",
      "        [-4.9026e-01, -7.5875e-01,  1.0804e-01,  1.1867e+00,  2.7920e-01,\n",
      "         -2.6196e-01, -2.0849e-01,  6.0679e-01,  6.5696e-01, -4.4379e-01],\n",
      "        [ 4.3873e-01, -3.6968e-01,  5.1434e-02, -1.0718e-01, -1.2394e-01,\n",
      "          2.1357e-01, -7.9625e-01, -3.4813e-01,  3.4120e-01, -9.0317e-02],\n",
      "        [ 3.5936e-01, -3.5802e-01, -2.4486e-02,  1.7729e-01, -4.7971e-01,\n",
      "          4.0644e-01,  5.4667e-01, -2.3145e-01, -3.0967e-01, -1.5753e-01],\n",
      "        [ 3.0623e-01, -2.3243e-01,  5.6851e-01, -3.2185e-02, -6.6859e-01,\n",
      "         -6.1670e-01, -2.2121e-01, -5.6025e-02, -7.1131e-01, -1.1972e-01],\n",
      "        [-1.9065e-01, -1.7435e-01, -8.3650e-02,  2.1777e-01, -6.7440e-01,\n",
      "         -3.2672e-01,  4.9268e-01, -2.0245e-01,  6.1524e-01, -5.8126e-02],\n",
      "        [ 2.3778e-01, -3.2154e-01, -8.8631e-01, -6.1672e-01,  6.0761e-01,\n",
      "          2.2824e-01, -4.5523e-01, -4.7686e-01,  2.6879e-01,  3.2509e-01],\n",
      "        [ 2.4148e-01, -6.7121e-01, -1.9931e-01, -9.9736e-01,  4.6343e-01,\n",
      "          9.7545e-02, -1.3113e-02,  9.6804e-01,  2.7133e-01,  2.0726e-01],\n",
      "        [-2.9366e-01, -2.3324e-01,  5.0375e-01,  3.1166e-02, -1.7948e-01,\n",
      "         -1.6711e-01,  3.2154e-01, -4.9529e-01, -3.0785e-01,  4.2780e-02],\n",
      "        [-3.5519e-02, -5.9368e-01, -1.2322e+00, -1.1859e-01,  4.8074e-01,\n",
      "         -9.4627e-01, -4.3520e-01, -4.6319e-01,  3.1672e-01,  2.3010e-01],\n",
      "        [-3.6043e-01, -4.0407e-01,  1.0984e+00, -3.9156e-01,  7.2223e-02,\n",
      "          2.7665e-01, -4.0083e-01,  5.3737e-01,  6.3922e-01,  1.6160e-01],\n",
      "        [-1.3881e+00,  1.1085e+00,  7.6456e-01, -3.5471e-01, -1.8076e-01,\n",
      "          4.5397e-01, -8.4596e-01,  1.7545e-01,  1.1131e+00, -3.0176e-01],\n",
      "        [-1.4341e+00,  1.6365e+00, -6.7499e-01, -6.5110e-01,  9.2147e-01,\n",
      "         -3.3932e-01, -6.7537e-01,  3.6987e-02,  2.4847e-01, -1.9461e-01],\n",
      "        [ 3.2204e-02, -3.5673e-01, -3.4387e-01,  2.3234e-01, -2.1698e-01,\n",
      "          6.1655e-01, -2.5544e-01, -3.3778e-01, -3.5893e-01, -3.2630e-01],\n",
      "        [-8.1016e-01, -4.6990e-01,  1.5804e-01,  6.8304e-01,  1.3296e-01,\n",
      "          6.1031e-01,  5.8224e-01,  1.5771e-02,  3.9365e-01, -6.4736e-01],\n",
      "        [-5.1891e-01, -5.5640e-02, -3.6787e-01, -3.4366e-01, -3.2506e-01,\n",
      "         -8.0711e-02, -4.8123e-01, -8.7804e-02, -2.3988e-01,  3.0712e-01],\n",
      "        [ 4.8100e-01,  2.5172e-01,  6.3193e-01,  1.6480e-01, -9.3139e-02,\n",
      "         -2.1805e-01,  2.7455e-02, -3.8774e-01,  4.0452e-01,  2.6208e-01],\n",
      "        [ 9.2348e-02,  1.3671e+00,  7.7828e-01,  1.8330e-01,  3.9531e-01,\n",
      "         -3.5605e-01, -2.4772e-01, -1.7480e-01, -4.5864e-01, -7.4571e-01],\n",
      "        [ 2.7604e-01, -1.4863e-01, -3.7831e-01,  3.2233e-01, -7.0353e-02,\n",
      "         -6.5545e-01, -5.0781e-01, -3.6450e-01, -1.6539e-01,  6.1210e-01],\n",
      "        [-3.5468e-02,  7.5296e-02,  3.9701e-01, -2.4099e-01,  2.6328e-01,\n",
      "          4.8023e-02,  2.5186e-01,  6.4773e-01, -4.0940e-01, -7.2027e-01],\n",
      "        [ 4.0135e-01, -6.6069e-01,  3.9113e-01, -3.9995e-01,  1.5383e-01,\n",
      "          3.0183e-01, -4.5274e-01, -7.8149e-01, -1.1674e-01,  4.1357e-01],\n",
      "        [-3.4483e-01, -4.5947e-01, -4.9916e-01,  4.4617e-01,  1.5257e-01,\n",
      "          4.0232e-01, -2.5349e-01, -6.3910e-02, -4.4795e-01,  1.5567e-02],\n",
      "        [ 1.5750e-01,  5.7156e-01, -6.7559e-02,  3.7004e-01, -8.6389e-01,\n",
      "         -2.6355e-01,  9.0179e-02, -3.7434e-02, -5.3730e-01, -2.3420e-01],\n",
      "        [-9.4505e-01,  1.6068e-01,  6.7816e-01, -6.4007e-02,  6.0592e-01,\n",
      "          1.6926e+00, -7.7391e-01, -7.4054e-01,  5.2129e-01,  2.2731e-01],\n",
      "        [ 1.6880e-01, -2.2210e-02, -4.7769e-01,  2.8997e-01, -5.3153e-01,\n",
      "         -7.8129e-01,  2.3462e-02, -9.2694e-02, -5.6477e-01,  8.6646e-01]],\n",
      "       dtype=torch.float64))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'adjacency_m' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4576/1877000874.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0madj_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_neigh_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madjacency_m\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'adjacency_m' is not defined"
     ]
    }
   ],
   "source": [
    "#Generating new graph\n",
    "\n",
    "random_graph = featureless_random_graph(100)\n",
    "print(random_graph)\n",
    "\n",
    "adj_list = get_neigh_list(adjacency_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec8e1efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  9.],\n",
      "        [ 1.,  9.],\n",
      "        [ 2., 10.],\n",
      "        [ 3.,  7.],\n",
      "        [ 4., 11.],\n",
      "        [ 5.,  5.],\n",
      "        [ 6., 11.],\n",
      "        [ 7.,  9.],\n",
      "        [ 8., 11.],\n",
      "        [ 9., 12.],\n",
      "        [10., 15.],\n",
      "        [11.,  9.],\n",
      "        [12.,  7.],\n",
      "        [13.,  7.],\n",
      "        [14., 14.],\n",
      "        [15., 10.],\n",
      "        [16.,  7.],\n",
      "        [17., 11.],\n",
      "        [18.,  7.],\n",
      "        [19.,  8.],\n",
      "        [20.,  9.],\n",
      "        [21., 14.],\n",
      "        [22., 10.],\n",
      "        [23., 11.],\n",
      "        [24.,  9.],\n",
      "        [25.,  8.],\n",
      "        [26.,  6.],\n",
      "        [27., 13.],\n",
      "        [28., 11.],\n",
      "        [29., 13.],\n",
      "        [30., 12.],\n",
      "        [31.,  8.],\n",
      "        [32., 11.],\n",
      "        [33., 12.],\n",
      "        [34., 15.],\n",
      "        [35., 12.],\n",
      "        [36.,  8.],\n",
      "        [37., 10.],\n",
      "        [38., 13.],\n",
      "        [39., 10.],\n",
      "        [40., 16.],\n",
      "        [41., 13.],\n",
      "        [42.,  6.],\n",
      "        [43.,  4.],\n",
      "        [44.,  7.],\n",
      "        [45., 11.],\n",
      "        [46., 10.],\n",
      "        [47., 10.],\n",
      "        [48.,  5.],\n",
      "        [49., 10.],\n",
      "        [50.,  6.],\n",
      "        [51., 11.],\n",
      "        [52., 10.],\n",
      "        [53., 18.],\n",
      "        [54., 12.],\n",
      "        [55.,  8.],\n",
      "        [56., 15.],\n",
      "        [57., 12.],\n",
      "        [58., 10.],\n",
      "        [59., 11.],\n",
      "        [60.,  9.],\n",
      "        [61., 13.],\n",
      "        [62., 17.],\n",
      "        [63., 10.],\n",
      "        [64., 11.],\n",
      "        [65., 12.],\n",
      "        [66., 10.],\n",
      "        [67.,  8.],\n",
      "        [68., 12.],\n",
      "        [69., 12.],\n",
      "        [70., 12.],\n",
      "        [71., 14.],\n",
      "        [72.,  7.],\n",
      "        [73., 11.],\n",
      "        [74.,  7.],\n",
      "        [75.,  8.],\n",
      "        [76., 10.],\n",
      "        [77., 14.],\n",
      "        [78.,  9.],\n",
      "        [79.,  5.],\n",
      "        [80.,  8.],\n",
      "        [81., 12.],\n",
      "        [82., 11.],\n",
      "        [83., 10.],\n",
      "        [84.,  3.],\n",
      "        [85.,  6.],\n",
      "        [86., 11.],\n",
      "        [87.,  8.],\n",
      "        [88., 13.],\n",
      "        [89., 10.],\n",
      "        [90., 12.],\n",
      "        [91.,  9.],\n",
      "        [92., 13.],\n",
      "        [93., 12.],\n",
      "        [94.,  9.],\n",
      "        [95.,  7.],\n",
      "        [96., 12.],\n",
      "        [97., 15.],\n",
      "        [98.,  7.],\n",
      "        [99.,  6.]])\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import networkx as nx\n",
    "\n",
    "d = utils.featureless_random_graph(100)\n",
    "print(d[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
